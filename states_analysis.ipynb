{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from importlib import reload\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import states_analysis_tools as sat\n",
    "reload(sat)\n",
    "import h5py\n",
    "import acme\n",
    "from acme import ParallelMap\n",
    "import superlet as slt\n",
    "\n",
    "#read in the data, set general variables\n",
    "project_folder='attention_paper_dataset'\n",
    "species_data = sat.read_dfs(folder=project_folder)\n",
    "\n",
    "project='behaviour_paper'\n",
    "\n",
    "colormap_states = ListedColormap(['#b15c83ff','#519296ff','#43dfcdff','#f394aeff'])\n",
    "colormap_states_speed = ListedColormap(['#5d5f6cff','#d6d7dcff'])\n",
    "colormap_states_accuracy = ListedColormap(['#f394aeff','#68d7c9ff'])\n",
    "\n",
    "sns.set_style(\"ticks\", {\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False\n",
    "})\n",
    "\n",
    "# Update tick thickness globally via Matplotlib\n",
    "plt.rcParams.update({\n",
    "    \"axes.linewidth\": 1.5,\n",
    "    \"xtick.major.width\": 1.2,  # Thickness of major ticks on the x-axis\n",
    "    \"ytick.major.width\": 1.2,  # Thickness of major ticks on the y-axis\n",
    "    \"xtick.minor.width\": 1.0,  # Thickness of minor ticks on the x-axis\n",
    "    \"ytick.minor.width\": 1.0,  # Thickness of minor ticks on the y-axis\n",
    "    \"xtick.major.size\": 6,     # Length of major ticks on the x-axis\n",
    "    \"ytick.major.size\": 6,     # Length of major ticks on the y-axis\n",
    "    \"xtick.minor.size\": 4,     # Length of minor ticks on the x-axis\n",
    "    \"ytick.minor.size\": 4,     # Length of minor ticks on the y-axis\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPROCESSESSING\n",
    "species_data = sat.read_dfs(folder=project_folder)\n",
    "selected_metrics = ['Session', 'TrialDuration', 'MorphTarget', 'MorphDistractor', 'Correct', 'Wrong', 'Prec', 'Bias', 'RT', 'SpeedMean', 'PL']\n",
    "\n",
    "for species in species_data:\n",
    "    sessions = species_data[species].copy()\n",
    "    sessions_list = []\n",
    "\n",
    "    for isesh, sesh in enumerate(sessions):\n",
    "        # Initial setup\n",
    "        sesh = sesh.loc[:, selected_metrics].copy()\n",
    "        sesh = sesh.dropna().reset_index(drop=True)\n",
    "        \n",
    "        # Remove learning period for humans\n",
    "        if species == 'human':\n",
    "            cutoff = sat.find_combined_learning_cutoff(sesh, window=10, threshold=0.5, consecutive=10)\n",
    "            sesh = sesh.iloc[cutoff:].reset_index(drop=True)\n",
    "        \n",
    "        # Remove outlier trials based on duration and path length\n",
    "        mask = (sesh['TrialDuration'] <= 10) & (sesh['PL'] >= 250)\n",
    "        sesh = sesh[mask].reset_index(drop=True)\n",
    "        \n",
    "        # Flip RT and Bias\n",
    "        sesh['RT'] = -sesh['RT']\n",
    "        sesh['Bias'] = -sesh['Bias']\n",
    "        \n",
    "        # Convert to numeric\n",
    "        sesh = sesh.apply(pd.to_numeric, errors='ignore')\n",
    "        \n",
    "        sessions_list.append(sesh)\n",
    "    \n",
    "    sat.save_dfs(sessions_list, project, species, savepath=None)\n",
    "\n",
    "preprocessed_data = sat.read_dfs(folder=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES\n",
    "preprocessed_data=sat.read_dfs(folder=project)\n",
    "\n",
    "accuracy_metrics=['Correct','Bias','Prec']\n",
    "speed_metrics=['RT','SpeedMean']\n",
    "training_metrics=accuracy_metrics+speed_metrics\n",
    "\n",
    "n_iter=1000\n",
    "n_workers = 100\n",
    "trials_per_worker = n_iter // n_workers\n",
    "\n",
    "lower=150 #slower 6\n",
    "upper=15 #faster 66\n",
    "steps = 200#100# \n",
    "\n",
    "fs = 1000 \n",
    "foi = np.linspace(fs/lower, fs/upper, steps)\n",
    "tpc=fs/foi\n",
    "c1 = 5\n",
    "ord = (1, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM\n",
    "preprocessed_data = sat.read_dfs(folder=project)\n",
    "for species in species_data:\n",
    "    sessions=preprocessed_data[species]\n",
    "    columns, values =sat.prepare_and_train_dataset(1,sessions, accuracy_metrics, speed_metrics, shuffle=False)\n",
    "    training_data = pd.DataFrame(values, columns=columns)\n",
    "    sat.save_pickle(training_data,f'training_data',species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUPERLETS\n",
    "metrics_to_slt=['States_Speed','States_Accuracy']##'States_Speed'#'SpeedMean'#'Combined_Speed'#,'RT_Raw','SpeedMean_Raw','Correct_Raw'\n",
    "for species in species_data:\n",
    "    for metric in metrics_to_slt:    \n",
    "        training_data = sat.load_pickle('training_data', species)\n",
    "        sesh_id=training_data['Session'].values\n",
    "        power_per_sesh=[]\n",
    "        for isesh in np.unique(sesh_id):\n",
    "            sesh=training_data[sesh_id==isesh].copy()\n",
    "            data=sesh[metric].values\n",
    "            result = slt.superlets(data[np.newaxis,:], fs, foi, c1, ord)\n",
    "            power_per_sesh.append(result)\n",
    "        power_per_sesh=np.concatenate(power_per_sesh,axis=1)\n",
    "\n",
    "        sat.save_pickle(power_per_sesh,f'power_{metric}',species)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM - SHUFFLED\n",
    "\n",
    "for species in species_data:\n",
    "    original_data = preprocessed_data[species]  # Load the original dataset\n",
    "    num_iterations = trials_per_worker * n_workers\n",
    "\n",
    "    # Shuffle and train HMM\n",
    "    client = acme.esi_cluster_setup(partition=\"8GBXS\", n_workers=n_workers)\n",
    "\n",
    "    # Pass a range variable just to iterate\n",
    "    with ParallelMap(sat.prepare_and_train_dataset, range(num_iterations), original_data, accuracy_metrics, speed_metrics,n_inputs=num_iterations) as pmap:\n",
    "        results = pmap.compute()\n",
    "\n",
    "    sat.save_pickle(results, f'acme_results', species)\n",
    "\n",
    "acme.cluster_cleanup()\n",
    "\n",
    "# LOAD HMM RESULTS\n",
    "for species in species_data:\n",
    "    results = sat.load_pickle(f'acme_results', species)\n",
    "\n",
    "    loaded_data = []\n",
    "    for result in results:\n",
    "        with h5py.File(result, 'r') as f:\n",
    "            columns=list(f['result_0'])\n",
    "            columns = [col.decode('utf-8')  for col in columns]\n",
    "            values=list(f['result_1'])\n",
    "            #columns, values = f['result_0']  # Load the training data from the result\n",
    "            df = pd.DataFrame(values, columns=columns)\n",
    "            loaded_data.append(df)\n",
    "\n",
    "    sat.save_pickle(loaded_data, f'shuffled_data', species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUPERLETS - SHUFFLED\n",
    "metrics_to_slt=['States_Speed','States_Accuracy']##'States_Speed'#'SpeedMean'#'Combined_Speed'#,'RT_Raw','SpeedMean_Raw','Correct_Raw'\n",
    "for species in species_data:\n",
    "    for metric in metrics_to_slt:  \n",
    "\n",
    "        data = sat.load_pickle('training_data', species)\n",
    "        sesh_id=data['Session'].values\n",
    "        \n",
    "        #Step 2: Load shuffled matrixes\n",
    "        shuffled_data=sat.load_pickle(f'shuffled_data',species)\n",
    "\n",
    "        shuffled_slt_input=[]\n",
    "        \n",
    "        # select the metric for superlets\n",
    "        for iteration in shuffled_data:   \n",
    "            shuffled_slt_input.append(iteration[metric].values)\n",
    "                \n",
    "        client = acme.esi_cluster_setup(partition=\"8GBXS\", n_workers=n_workers)\n",
    "\n",
    "        with ParallelMap(sat.compute_superlets_per_sesh,shuffled_slt_input,sesh_id,\n",
    "                        fs, foi, n_inputs=trials_per_worker * n_workers) as pmap:\n",
    "            slt_results = pmap.compute()\n",
    "        \n",
    "        sat.save_pickle(slt_results,f'results_slt_shuffled_{metr}',species)  \n",
    "    \n",
    "acme.cluster_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 1. C: EXAMPLE RUNNING PATHS  \n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "def norm_bar(data, map_name, max_value):\n",
    "    \"\"\"Creates a normalized colormap and scalar mappable object.\"\"\"\n",
    "    norm = Normalize(vmin=0.4, vmax=max_value)\n",
    "    cmap = plt.get_cmap(map_name)\n",
    "    mappable = ScalarMappable(norm=norm, cmap=cmap)\n",
    "    return cmap, mappable\n",
    "def plot_selected_running_paths(species_data, selected_indices_dict, metric='Prec', color_map='flare_r'):\n",
    "    \"\"\"\n",
    "    Plot running paths for different species with color-coded metrics using selected indices.\n",
    "    \"\"\"\n",
    "    # Set up the figure and colormap\n",
    "    max_value = 1.0\n",
    "    cmap, mappable = norm_bar(None, color_map, max_value)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    for i, (species, data) in enumerate(species_data.items()):\n",
    "        # Select appropriate session data\n",
    "        if species == 'human':\n",
    "            sesh = data[1] \n",
    "        if species == 'mouse':\n",
    "            sesh = data[10] \n",
    "        if species == 'monkey':\n",
    "            sesh = data[3] \n",
    "        sesh.dropna(inplace=True)\n",
    "        sesh.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Extract relevant data\n",
    "        distance = sesh['Distance']\n",
    "        boxsize = sesh.attrs['Metadata']['BoxSize']\n",
    "        precision_data = sesh[metric]\n",
    "        \n",
    "        # Filter paths based on correct trials\n",
    "        selected = sesh['Correct'].astype(bool)\n",
    "        precision_data = precision_data[selected].reset_index(drop=True)\n",
    "        paths = sesh['Location'][selected].reset_index(drop=True)\n",
    "        rt = sesh['RT'][selected].reset_index(drop=True)\n",
    "        \n",
    "        # Get indices for this species\n",
    "        try:\n",
    "            species_indices = np.load(f'selected_paths_{species}.npy', allow_pickle=True)\n",
    "            selected_paths = [paths[idx] for idx in species_indices]\n",
    "            selected_precision = [precision_data[idx] for idx in species_indices]\n",
    "            selected_rt = [rt[idx] for idx in species_indices]\n",
    "            selected_distance = [distance[idx] for idx in species_indices]\n",
    "        except:\n",
    "            print(f\"No selected paths found for {species}\")\n",
    "            continue\n",
    "            \n",
    "        # Print diagnostic information\n",
    "        print(f\"{species}: plotting {len(selected_paths)} selected paths\")\n",
    "        print(f\"{species} precision range: {min(selected_precision):.3f} to {max(selected_precision):.3f}\")\n",
    "        \n",
    "        # Plot paths\n",
    "        for ipath, (path, prec, rt_idx, dist) in enumerate(zip(selected_paths, selected_precision, selected_rt, selected_distance)):\n",
    "            # Find point nearest to target\n",
    "            x_near_targ = np.argmin(abs(path[:, 0] - dist - boxsize / 2))\n",
    "            \n",
    "            # Calculate coordinates based on species\n",
    "            if species == 'mouse':\n",
    "                y = path[:x_near_targ, 0] - path[:, 0][0]\n",
    "                x = path[:x_near_targ, 1] - path[:, 1][0]\n",
    "            else:\n",
    "                y = path[:x_near_targ, 0]\n",
    "                x = path[:x_near_targ, 1]\n",
    "            \n",
    "            # Plot path with normalized color\n",
    "            normalized_value = mappable.norm(prec)\n",
    "            axs[i].plot(x, y, color=cmap(normalized_value))\n",
    "            \n",
    "            # Plot reaction time point\n",
    "            axs[i].plot(\n",
    "                x[rt_idx],\n",
    "                y[rt_idx],\n",
    "                marker='o',\n",
    "                zorder=3,\n",
    "                markerfacecolor='None',\n",
    "                markeredgecolor='k'\n",
    "            )\n",
    "        \n",
    "        # Set plot limits and title\n",
    "        axs[i].set_xlim([-300, 300])\n",
    "        axs[i].set_ylim([-20, 600] if species == 'mouse' else [-20, 450])\n",
    "        axs[i].set_title(f\"{species.capitalize()} (n={len(selected_paths)})\")\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "    fig.colorbar(mappable, cax=cbar_ax, label=metric)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    sat.save_plot('cross_species',project, filename=f\"example_paths_{metric}.svg\")\n",
    "    plt.show()\n",
    "\n",
    "# Usage:\n",
    "plot_selected_running_paths(species_data, None)  # None for selected_indices_dict as we're loading from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 2. A: METRICS CORRELATION MATRIX\n",
    "all_species_data = []\n",
    "\n",
    "for species in species_data:\n",
    "    data = sat.load_pickle('training_data', species)\n",
    "    all_species_data.append(data[training_metrics])\n",
    "\n",
    "# Combine data for all species\n",
    "combined_data = pd.concat(all_species_data, ignore_index=True)\n",
    "\n",
    "# Compute correlation matrix on the combined data\n",
    "combined_corr_matrix = combined_data.corr()\n",
    "\n",
    "# Plot the correlation heatmap for combined data\n",
    "plt.figure(figsize=(6, 6))  # Adjust size as needed\n",
    "sns.heatmap(combined_corr_matrix, annot=False, cmap=\"BuPu\", vmin=-0.1, vmax=1)\n",
    "plt.title('Correlation Heatmap - Combined Species')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot if necessary\n",
    "sat.save_plot('cross_species', project, filename=\"metrics_correlation_heatmap_combined.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 2. B:  HIT RATE vs. RT\n",
    "sesh_counts=[]\n",
    "for i, species in enumerate(['human','monkey','mouse']):\n",
    "    data=sat.load_pickle(f'training_data',species)\n",
    "    data['RT']=data['RT']#*-1\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    x='Correct'\n",
    "    y='RT'    \n",
    "    plt.axhline(0, color='black', linewidth=0.8, zorder=-1)\n",
    "    plt.axvline(0, color='black', linewidth=0.8, zorder=-1)\n",
    "    sns.scatterplot(data=data,x=y,y=x,hue=data['States'],palette=colormap_states, alpha=1, legend=False)\n",
    "    plt.xlim([-6.0,6])\n",
    "    plt.ylim([-5.5,2.5])\n",
    "\n",
    "    plt.title(f'{x} vs {y}, {species}')\n",
    "    sat.save_plot('cross_species', project, filename=f\"cloud_hi_vs_rt_{species}.svg\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 2. C: STATE MEANS\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "for idx, species in enumerate(species_data):\n",
    "    data = sat.load_pickle('training_data', species)\n",
    "    states = data['States']\n",
    "\n",
    "    for state in range(4):\n",
    "        mean_values = np.mean(data[training_metrics][states == state], axis=0)\n",
    "        error_values = np.std(data[training_metrics][states == state], axis=0)\n",
    "        x_values = range(len(mean_values))  \n",
    "\n",
    "        # Use errorbar instead of plot to include error bars\n",
    "        axs[idx].errorbar(x_values, mean_values, yerr=error_values, label=f'State {state}', fmt='-o',elinewidth=1,\n",
    "            capsize=3,\n",
    "            linewidth=3,  # Thicker line\n",
    "            markersize=7, color=colormap_states.colors[state])\n",
    "        \n",
    "    axs[idx].set_ylim([-2.5, 2.5])\n",
    "    axs[idx].set_xticks(x_values)\n",
    "    axs[idx].set_xticklabels(training_metrics)\n",
    "    axs[idx].set_title(species)\n",
    "    axs[idx].axhline(0, color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "sat.save_plot('cross_species',project,filename=f\"means_species.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 3. A: STATES TIMESERIES\n",
    "for i, species in enumerate(['human','monkey','mouse']):\n",
    "    data=sat.load_pickle(f'training_data',species)\n",
    "    sat.plot_states_timeseries(data,colormap_states, species)\n",
    "    sat.save_plot('cross_species', project, filename=f\"example_sessions_{species}.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 3. B: STATES PROPORTION\n",
    "# Group by session and calculate the frequency of each state in each session\n",
    "sesh_counts=[]\n",
    "for i, species in enumerate(['human','monkey','mouse']):\n",
    "    data=sat.load_pickle(f'training_data',species)\n",
    "    state_session_counts = data['States'].value_counts(normalize=True).sort_index()#.unstack(fill_value=0)\n",
    "    sesh_counts.append(state_session_counts)\n",
    "all_sp_counts = pd.concat(sesh_counts,axis=1).T\n",
    "# Plotting the distribution of states across sessions\n",
    "plt.figure(figsize=(6, 5))\n",
    "all_sp_counts.plot(kind='bar', stacked=True, colormap=colormap_states, ax=plt.gca())\n",
    "plt.gca().set_xticklabels([])\n",
    "plt.legend(title='State', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "sat.save_plot('cross_species', project, filename=f\"state_proportion.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 3. C: EARLY VS. LATE STATE PROPORTION\n",
    "# List of species to process\n",
    "species_data = ['human', 'monkey', 'mouse']\n",
    "\n",
    "# List of state types to process\n",
    "state_types = ['States', 'States_Accuracy', 'States_Speed']\n",
    "\n",
    "# Loop through each state type to create a separate figure for each\n",
    "for state_type in state_types:\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(3, 8), sharex=True)  # Create subplots with shared x-axis\n",
    "\n",
    "    for idx, species in enumerate(species_data):\n",
    "        data = sat.load_pickle(f'training_data', species)\n",
    "\n",
    "        early = []\n",
    "        late = []\n",
    "        for isesh in np.unique(data['Session']):\n",
    "            sesh = data[data['Session'] == isesh]\n",
    "            bin1 = sesh[:len(sesh) // 2][state_type].value_counts().sort_index()\n",
    "            bin1 = bin1 / bin1.sum()\n",
    "\n",
    "            bin2 = sesh[len(sesh) // 2:][state_type].value_counts().sort_index()\n",
    "            bin2 = bin2 / bin2.sum()\n",
    "\n",
    "            early.append(bin1)\n",
    "            late.append(bin2)\n",
    "\n",
    "        # Compute average proportions and SEM\n",
    "        early_df = pd.DataFrame(early).fillna(0)\n",
    "        late_df = pd.DataFrame(late).fillna(0)\n",
    "\n",
    "        average_early = early_df.mean()-np.min([early_df.mean(),late_df.mean()])\n",
    "        average_late = late_df.mean()-np.min([early_df.mean(),late_df.mean()])\n",
    "\n",
    "        # Calculate SEM for error bars\n",
    "        sem_early = early_df.std() / np.sqrt(early_df.count())\n",
    "        sem_late = late_df.std() / np.sqrt(late_df.count())\n",
    "\n",
    "        # Plotting with the specified style on the current axis\n",
    "        for i in average_early.index:  # Loop over existing states\n",
    "            axs[idx].errorbar(\n",
    "                [0, 1],\n",
    "                [average_early[i], average_late[i]],\n",
    "                yerr=[sem_early[i], sem_late[i]],\n",
    "                linestyle='-',    # Line with markers\n",
    "                marker='o',       # Circle markers\n",
    "                elinewidth=1,     # Error bar line width\n",
    "                capsize=3,        # Error bar cap size\n",
    "                linewidth=3,      # Thicker line\n",
    "                markersize=7,     # Larger markers\n",
    "                color=sns.color_palette(colormap_states.colors)[int(i)], \n",
    "                label=f'State {int(i)}'\n",
    "            )\n",
    "\n",
    "        axs[idx].set_ylim([-0.05,0.30])\n",
    "        axs[idx].set_xlim([-0.2,1.2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'{state_type}', y=1.02)  # Set a super title for each figure\n",
    "    \n",
    "    sat.save_plot('cross_species', project, filename=f\"early_late_{state_type}.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 3. C: STATS\n",
    "\n",
    "# List of species to process\n",
    "species_data = ['human', 'monkey', 'mouse']\n",
    "\n",
    "# Initialize dictionaries to store p-values for each species and each state measure\n",
    "all_species_p_values = {\n",
    "    'States': {},\n",
    "    'States_Speed': {},\n",
    "    'States_Accuracy': {}\n",
    "}\n",
    "\n",
    "# Loop through each species\n",
    "for species in species_data:\n",
    "    # Load data and shuffled data for the species\n",
    "    data = sat.load_pickle(f'training_data', species)\n",
    "    shuffled = sat.load_pickle(f'shuffled_data', species)  # List of shuffled DataFrames\n",
    "\n",
    "    for state_type in ['States', 'States_Speed', 'States_Accuracy']:\n",
    "        early = []\n",
    "        late = []\n",
    "\n",
    "        # Calculate observed differences for the current state type\n",
    "        for isesh in np.unique(data['Session']):\n",
    "            sesh = data[data['Session'] == isesh]\n",
    "            bin1 = sesh[:len(sesh) // 2][state_type].value_counts().sort_index()\n",
    "            bin1 = bin1 / bin1.sum()\n",
    "\n",
    "            bin2 = sesh[len(sesh) // 2:][state_type].value_counts().sort_index()\n",
    "            bin2 = bin2 / bin2.sum()\n",
    "\n",
    "            early.append(bin1)\n",
    "            late.append(bin2)\n",
    "\n",
    "        early_df = pd.DataFrame(early).fillna(0)\n",
    "        late_df = pd.DataFrame(late).fillna(0)\n",
    "\n",
    "        average_early = early_df.mean()\n",
    "        average_late = late_df.mean()\n",
    "\n",
    "        # Calculate the observed differences between early and late for the current state type\n",
    "        observed_diffs = {i: average_early[i] - average_late[i] for i in average_early.index}\n",
    "\n",
    "        # Initialize dictionary to store permutation differences\n",
    "        perm_diffs = {i: [] for i in average_early.index}\n",
    "\n",
    "        # Loop through each shuffled DataFrame\n",
    "        for shuffled_df in shuffled:\n",
    "            early = []\n",
    "            late = []\n",
    "            for isesh in np.unique(shuffled_df['Session']):\n",
    "                sesh = shuffled_df[shuffled_df['Session'] == isesh]\n",
    "                bin1 = sesh[:len(sesh) // 2][state_type].value_counts().sort_index()\n",
    "                bin1 = bin1 / bin1.sum()\n",
    "\n",
    "                bin2 = sesh[len(sesh) // 2:][state_type].value_counts().sort_index()\n",
    "                bin2 = bin2 / bin2.sum()\n",
    "\n",
    "                early.append(bin1)\n",
    "                late.append(bin2)\n",
    "\n",
    "            # Calculate means for shuffled data\n",
    "            early_df = pd.DataFrame(early).fillna(0)\n",
    "            late_df = pd.DataFrame(late).fillna(0)\n",
    "\n",
    "            average_early_perm = early_df.mean()\n",
    "            average_late_perm = late_df.mean()\n",
    "\n",
    "            # Calculate the permutation differences\n",
    "            for i in average_early.index:\n",
    "                perm_diffs[i].append(average_early_perm[i] - average_late_perm[i])\n",
    "\n",
    "        # Calculate p-values by comparing observed differences to permutation differences\n",
    "        p_values = {}\n",
    "        for i in average_early.index:\n",
    "            # Count how many permuted differences are as extreme or more extreme than observed\n",
    "            extreme_count = np.sum(np.abs(perm_diffs[i]) >= np.abs(observed_diffs[i]))\n",
    "            p_values[i] = extreme_count / len(perm_diffs[i])\n",
    "\n",
    "        # Store p-values for this species and state type\n",
    "        all_species_p_values[state_type][species] = p_values\n",
    "\n",
    "# Print p-values for all species and state types\n",
    "for state_type, species_p_vals in all_species_p_values.items():\n",
    "    print(f'P-values for {state_type}:')\n",
    "    for species, p_vals in species_p_vals.items():\n",
    "        print(f'  {species}:')\n",
    "        for state, p_val in p_vals.items():\n",
    "            print(f'    State {state}: p-value = {p_val:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 4. A: DWELL TIMES\n",
    "# List of species to process\n",
    "species_data = ['human', 'monkey', 'mouse']\n",
    "\n",
    "# Define state types to process\n",
    "state_types = ['States', 'States_Accuracy', 'States_Speed']\n",
    "\n",
    "# Loop through each state type to create a separate figure for each\n",
    "for state_type in state_types:\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(4, 11), sharex=True)  # Create subplots for each species\n",
    "\n",
    "    for i, species in enumerate(species_data):\n",
    "        data = sat.load_pickle(f'training_data', species)\n",
    "        state_durations = data['TrialDuration']\n",
    "        states = data[state_type]  # Use the current state type\n",
    "\n",
    "        in_seconds = False\n",
    "        z, p, d, values = sat.rle_with_delays(states)  # Run-length encoding, etc.\n",
    "\n",
    "        # Define time format and limits\n",
    "        time_format = 'trials'\n",
    "        upper_dwell = 35  # Set the upper dwell time limit\n",
    "        upper_delay = 100  # Not used but kept for consistency\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        plot_data = []\n",
    "        unique_states = np.unique(values)  # Identify the unique states present in the data\n",
    "\n",
    "        for j in unique_states:\n",
    "            plot_data.extend([(str(j), val) for val in z[values == j]])\n",
    "\n",
    "        df = pd.DataFrame(plot_data, columns=['State', 'Dwell Time'])\n",
    "\n",
    "        # Plotting on the corresponding subplot\n",
    "        sns.stripplot(\n",
    "            x='State', y='Dwell Time', data=df, \n",
    "            palette=colormap_states.colors, size=4, alpha=0.3, \n",
    "            jitter=0.15, edgecolor='black', linewidth=0.5, ax=axs[i]\n",
    "        )\n",
    "        sns.boxplot(\n",
    "            x='State', y='Dwell Time', data=df, \n",
    "            palette=colormap_states.colors, showfliers=False, width=0.6, ax=axs[i]\n",
    "        )\n",
    "\n",
    "        # Set the y-limit and y-ticks divisible by 10\n",
    "        axs[i].set_ylim([0, 40])\n",
    "        axs[i].set_yticks(range(0, 45, 10))  # Ticks at 0, 10, 20, 30\n",
    "        axs[i].set_xlim([-0.5, max(unique_states) + 0.5])\n",
    "        \n",
    "    # Adjust layout and remove excess spines\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    \n",
    "    # Save plots in different formats\n",
    "    sat.save_plot('cross_species', project, filename=f\"dwell_times_{state_type}.svg\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 4. A: STATS\n",
    "from scipy.stats import kruskal\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "in_seconds = False\n",
    "if in_seconds:\n",
    "    time_format = 'seconds'\n",
    "else:\n",
    "    time_format = 'trials'\n",
    "\n",
    "# Collect dwell times for each species\n",
    "species_dwell_times = {}\n",
    "\n",
    "for species in species_data:\n",
    "    data = sat.load_pickle('training_data', species)\n",
    "    states = data['States']\n",
    "    state_durations = data['TrialDuration']\n",
    "    z, p, d, values = sat.rle_in_seconds(states, state_durations, time_seconds=in_seconds)\n",
    "\n",
    "    # Store dwell times for each species\n",
    "    species_dwell_times[species] = z\n",
    "\n",
    "# Print average dwell times across all states per species\n",
    "average_dwell_times = {species: np.mean(dwell_times) for species, dwell_times in species_dwell_times.items()}\n",
    "print(f\"Average dwell times {time_format}:\", average_dwell_times)\n",
    "\n",
    "# Perform Kruskal-Wallis test across species\n",
    "dwell_time_values = list(species_dwell_times.values())\n",
    "h_stat, p_value = kruskal(*dwell_time_values)\n",
    "\n",
    "print(f\"Kruskal-Wallis H-statistic: {round(h_stat, 2)}, p-value: {round(p_value, 2)}\")\n",
    "\n",
    "# Perform post-hoc Dunn's test with Bonferroni correction if Kruskal-Wallis is significant\n",
    "if p_value < 0.05:\n",
    "    species_names = list(species_dwell_times.keys())\n",
    "    posthoc = sp.posthoc_dunn(dwell_time_values, p_adjust='bonferroni')\n",
    "\n",
    "    # Set the species names as indices and columns for the post-hoc result\n",
    "    posthoc.index = species_names\n",
    "    posthoc.columns = species_names\n",
    "    \n",
    "    print(\"Post-hoc pairwise comparisons (Dunn's test with Bonferroni correction):\\n\",round(posthoc, 2) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 4. B: TRANSITION MATRICES\n",
    "\n",
    "for species in species_data:\n",
    "    data = sat.load_pickle(f'training_data', species)\n",
    "    transision_matrix = sat.create_transition_matrix(data['States'].values, minus_diag=True, window=1)\n",
    "    fig, ax = plt.subplots(ncols=1, figsize=(4, 4))\n",
    "\n",
    "    # Plot the heatmap with annotations, limit range, and format numbers to 2 decimals\n",
    "    sns.heatmap(\n",
    "        transision_matrix, \n",
    "        ax=ax, \n",
    "        annot=True, \n",
    "        fmt=\".2f\",        # Format numbers to 2 decimal places\n",
    "        cmap='Greys', \n",
    "        vmax=0.8, \n",
    "        cbar=False        # Disable colorbar if not needed\n",
    "    )\n",
    "    # Remove ticks and labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    # Save and display the plot\n",
    "    sat.save_plot('cross_species', project, filename=f\"transition_matrix_{species}.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 4. B: STATS\n",
    "\n",
    "p_values = {}\n",
    "\n",
    "for species in species_data:\n",
    "    # Load original data and compute the transition matrix\n",
    "    data = sat.load_pickle('training_data', species)\n",
    "    original_states = data['States'].values\n",
    "    original_transition_matrix = sat.create_transition_matrix(original_states, minus_diag=True, window=1)\n",
    "    \n",
    "    # Compute transition matrices for 1000 shuffles using block shuffling\n",
    "    shuffled_transmats = []\n",
    "    for _ in range(1000):\n",
    "        shuffled_states = sat.shuffle_blocks(original_states)\n",
    "        shuffled_transition_matrix = sat.create_transition_matrix(shuffled_states, minus_diag=True, window=1)\n",
    "        shuffled_transmats.append(shuffled_transition_matrix)\n",
    "    mean_shuffled_transmat = np.mean(shuffled_transmats, axis=0)\n",
    "    \n",
    "    # Compute effect sizes and p-values\n",
    "    n_states = original_transition_matrix.shape[0]\n",
    "    p_matrix = np.zeros((n_states, n_states))\n",
    "    effect_size_matrix = np.zeros((n_states, n_states))\n",
    "    \n",
    "    for i in range(n_states):\n",
    "        for j in range(n_states):\n",
    "            shuffled_vals = np.array([mat[i, j] for mat in shuffled_transmats])\n",
    "            mean_shuffled = np.mean(shuffled_vals)\n",
    "            std_shuffled = np.std(shuffled_vals)\n",
    "            \n",
    "            # Compute two-sided p-value based on absolute differences\n",
    "            original_val = original_transition_matrix[i, j]\n",
    "            p_matrix[i, j] = np.mean(\n",
    "                np.abs(shuffled_vals - mean_shuffled) >= \n",
    "                np.abs(original_val - mean_shuffled)\n",
    "            )\n",
    "\n",
    "            # Compute Cohen's d effect size\n",
    "            if std_shuffled > 0:\n",
    "                effect_size_matrix[i, j] = (original_val - mean_shuffled) / std_shuffled\n",
    "            else:\n",
    "                effect_size_matrix[i, j] = 0\n",
    "    \n",
    "    # Apply FDR correction to the p-values\n",
    "    flat_p = p_matrix.flatten()\n",
    "    rejected, p_corrected, _, _ = multipletests(flat_p, alpha=0.05, method='fdr_bh')#bonferroni\n",
    "    significant = rejected.reshape(p_matrix.shape)\n",
    "    # Create annotations for effect sizes with significance markers\n",
    "    effect_size_annot = np.empty_like(effect_size_matrix, dtype=object)\n",
    "    for i in range(n_states):\n",
    "        for j in range(n_states):\n",
    "            effect_size_annot[i, j] = f\"{effect_size_matrix[i,j]:.1f}\" + (\"*\" if significant[i,j] else \"\")\n",
    "\n",
    "    # Plot the heatmaps\n",
    "    fig, axes = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "    \n",
    "    # Original transition matrix\n",
    "    sns.heatmap(original_transition_matrix, ax=axes[0], annot=True, fmt='.2f', cmap='Blues',vmax=0.6)\n",
    "    axes[0].set_title(f'Original Transition Matrix\\n{species}')\n",
    "    \n",
    "    # Mean shuffled transition matrix\n",
    "    sns.heatmap(mean_shuffled_transmat, ax=axes[1], annot=True, fmt='.2f', cmap='Blues',vmax=0.6)\n",
    "    axes[1].set_title(f'Mean Shuffled Matrix\\n{species}')\n",
    "    \n",
    "    # Effect size matrix with significance markers\n",
    "    # Using diverging colormap centered at 0 for effect sizes\n",
    "    sns.heatmap(effect_size_matrix, ax=axes[2], \n",
    "                annot=effect_size_annot, fmt='', \n",
    "                cmap='RdBu_r', center=0,\n",
    "                vmin=-3, vmax=3)  # Limiting to typical Cohen's d range\n",
    "    axes[2].set_title(f'Effect Sizes (Cohen\\'s d)\\n* = significant, {species}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 5. A-B: POWER SPECTRUM OF STATE SEQUENCE\n",
    "metrics_to_slt=['States_Speed','States_Accuracy']##'States_Speed'#'SpeedMean'#'Combined_Speed'#,'RT_Raw','SpeedMean_Raw','Correct_Raw'\n",
    "for metr in metrics_to_slt:   \n",
    "    fig, axs = plt.subplots(3, 1, figsize=(5, 10), sharex=True)\n",
    "    for i, species in enumerate(species_data):\n",
    "        power = sat.load_pickle(f'power_{metr}', species)\n",
    "        avg_power_fr = np.mean(power, axis=1)\n",
    "\n",
    "        slt_results = sat.load_pickle(f'results_slt_shuffled_{metr}', species)\n",
    "        means = []\n",
    "        for result in slt_results:\n",
    "            with h5py.File(result, 'r') as f:\n",
    "                data = f['result_0']\n",
    "                means.append(np.mean(data, axis=1))\n",
    "\n",
    "        means = np.array(means)\n",
    "        mean_shuffled_power = np.mean(means, axis=0)\n",
    "        q5, q95 = np.percentile(means, [2.5, 97.5], axis=0)\n",
    "        \n",
    "        axs[i].plot(fs/foi, avg_power_fr, label='Actual Power', color='black')\n",
    "        axs[i].plot(fs/foi, mean_shuffled_power, color='gray', linestyle='--', label='Shuffled Mean')\n",
    "        axs[i].fill_between(fs/foi, q5, q95, color='gray', alpha=0.3, label='95 CI')\n",
    "        axs[i].set_xlim(15, 100)\n",
    "        axs[i].set_ylim(0.003,0.0155)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    sat.save_plot('cross_species', project, filename=f\"avg_pw_fr_line_states_{metr}.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE 6. DIFFICULTY EFFECT\n",
    "\n",
    "# Define the species to process\n",
    "species_data = ['human', 'monkey', 'mouse']\n",
    "metric='States_Speed'#'States_Accuracy'#\n",
    "num_permutations = 1000  # Set the number of permutations\n",
    "\n",
    "# Initialize a dictionary to store difficulty differences for each species and transition type\n",
    "transition_diffs_all_species = {\n",
    "    species: {(from_state, to_state): [] for from_state in range(4) for to_state in range(4) if from_state != to_state} \n",
    "    for species in species_data\n",
    "}\n",
    "\n",
    "# Process the data for each species\n",
    "for species in species_data:\n",
    "    data = sat.load_pickle(f'training_data', species)\n",
    "    states = data[metric]\n",
    "    correct = data['Correct']  # Performance data\n",
    "    z, p, values = sat.rle(states)\n",
    "    all_morphs = []\n",
    "\n",
    "    for isesh in data['Session'].unique():\n",
    "        this_sesh = data['Session'] == isesh\n",
    "        session_data = data[this_sesh]\n",
    "        morphs = session_data['MorphTarget'].apply(sat.fold_morph_values)\n",
    "        \n",
    "        # Normalize difficulty per session\n",
    "        mean_difficulty = morphs.mean()\n",
    "        all_morphs.extend(morphs - mean_difficulty)\n",
    "\n",
    "    # Iterate through each state change position\n",
    "    for idx in range(1, len(p)):\n",
    "        from_state = values[idx - 1]\n",
    "        to_state = values[idx]\n",
    "        pos = p[idx]\n",
    "\n",
    "        # Skip self-transitions\n",
    "        if from_state == to_state:\n",
    "            continue\n",
    "\n",
    "        # Define the window around the state change (+-10 trials)\n",
    "        start = pos - 10\n",
    "        end = pos + 10\n",
    "\n",
    "        # Ensure that the window is fully within the range of all_morphs\n",
    "        if start < 0 or end >= len(all_morphs):\n",
    "            continue  # Skip this transition if the window is out of bounds\n",
    "\n",
    "        # Get the morph difficulties within the window\n",
    "        window_difficulties = all_morphs[start:end + 1]\n",
    "        \n",
    "        # Store the difficulty differences based on transition type (from_state -> to_state)\n",
    "        transition_diffs_all_species[species][(from_state, to_state)].append(window_difficulties)\n",
    "\n",
    "# Set up the plot\n",
    "num_species = len(species_data)\n",
    "num_transitions = 2  # Assuming 2 transitions: slow->fast and fast->slow\n",
    "fig, axes = plt.subplots(num_transitions, num_species, figsize=(8, 5))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.2)\n",
    "\n",
    "# Plot all transitions for each species and calculate the null distribution\n",
    "for col, species in enumerate(species_data):\n",
    "    transition_diffs = transition_diffs_all_species[species]\n",
    "    \n",
    "    # Add species name at the top of each column\n",
    "    axes[0, col].set_title(f'{species.capitalize()}', fontsize=14, pad=20)\n",
    "    \n",
    "    row = 0\n",
    "    for transition, diffs in transition_diffs.items():\n",
    "        if diffs:  # Only plot if there are transitions of that type\n",
    "            # Convert list of transitions (each transition is a list of 21 trials) into a numpy array\n",
    "            diffs_array = np.array(diffs)\n",
    "            \n",
    "            # Calculate the percentage of trials above the mean (positive trials) for each trial position\n",
    "            percent_positive_per_trial = np.mean(diffs_array >= 0, axis=0) * 100\n",
    "            \n",
    "            # Calculate null distribution by shuffling difficulty labels\n",
    "            null_distribution = sat.compute_null_distribution(diffs_array, num_permutations)\n",
    "            \n",
    "            # Calculate 95th percentile for each trial point\n",
    "            upper_threshold = np.percentile(null_distribution, 97.5 , axis=1)\n",
    "            lower_threshold = np.percentile(null_distribution, 2.5, axis=1)\n",
    "            \n",
    "            trials = np.arange(-10, 11)  # Define trial range (-10 to +10 around transition)\n",
    "\n",
    "            # Plot in the appropriate subplot\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Plot observed percentages\n",
    "            ax.plot(trials, percent_positive_per_trial, color='r', label=\"Observed\")\n",
    "            \n",
    "            # Plot null distribution upper and lower thresholds\n",
    "            ax.fill_between(trials, lower_threshold, upper_threshold, color='gray', alpha=0.3, label=\"95% Null\")\n",
    "            \n",
    "            # Plot chance line\n",
    "            ax.axhline(np.mean(percent_positive_per_trial), color='black', linestyle='--', label=\"Mean Observed\")  # Adjusted line\n",
    "            ax.axvline(0, color='black', linestyle='-')  # Transition line\n",
    "\n",
    "            # Set limits\n",
    "            ax.set_yticks([0, 50, 100])  # Set y-ticks\n",
    "            ax.set_ylim([0, 100])  # Set y-axis limits\n",
    "            ax.set_xlim([-10, 10])  # Set x-axis limits\n",
    "\n",
    "            row += 1  # Move to the next row for plotting\n",
    "\n",
    "# Adjust plot appearance\n",
    "for ax in axes.flat:\n",
    "    ax.label_outer()  # Hide outer labels but keep inner ticks\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "sat.save_plot('cross_species', project, filename=f\"difficulty_psth_{metric}.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE S.1. METRICS DISTRIBUTION\n",
    "\n",
    "n_species = 3\n",
    "n_metrics = 5\n",
    "\n",
    "plt.figure(figsize=(n_metrics * 2, n_species * 2))\n",
    "\n",
    "# Remove 'Correct' and 'Wrong' from selected metrics\n",
    "selected_metrics = ['Bias', 'Prec', 'RT', 'SpeedMean']\n",
    "\n",
    "data = {}\n",
    "\n",
    "# Concatenate sessions data for each species\n",
    "for isp, species in enumerate(species_data):\n",
    "    sessions = species_data[species].copy()\n",
    "    sesh_list = []   \n",
    "    for isesh, sesh in enumerate(sessions):\n",
    "        sesh = sesh.copy()\n",
    "        sesh = sesh.dropna().reset_index()\n",
    "        sesh = sesh.loc[:, selected_metrics + ['Correct', 'Wrong']]  # Keep Correct and Wrong only for Outcome calculation\n",
    "        sesh['RT'] = sesh['RT'] / 60  # Convert reaction time\n",
    "        sesh['Outcome'] = sesh['Correct'] * 2 + sesh['Wrong']  # Compute the Outcome based on Correct and Wrong\n",
    "        sesh_list.append(sesh)\n",
    "    data[species] = pd.concat(sesh_list)\n",
    "selected_metrics=['Outcome']+selected_metrics\n",
    "# Plot the distributions\n",
    "for isp, species in enumerate(species_data):\n",
    "    for i, column in enumerate(selected_metrics, 1):\n",
    "        ax = plt.subplot(n_species, n_metrics, isp * n_metrics + i)\n",
    "        \n",
    "        # Plot histograms split by 'Outcome'\n",
    "        sns.histplot(data[species], x=column, hue='Outcome',  kde=True, ax=ax, legend=(isp == 0 and i == 1))#multiple='stack',\n",
    "        \n",
    "        # Show x-axis label only for the bottom row\n",
    "        if isp == n_species - 1:\n",
    "            ax.set_xlabel(column)\n",
    "        else:\n",
    "            ax.set_xlabel('')\n",
    "\n",
    "        # Show y-axis label only for the first column\n",
    "        if i == 1:\n",
    "            ax.set_ylabel(species)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "            \n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "sat.save_plot('cross_species', project, filename=f\"metrics_distribution_by_outcome.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIG S.2. METRICS CORRELATIONS\n",
    "selected_metrics = ['Correct_Raw','Bias_Raw', 'Prec_Raw', 'RT_Raw', 'SpeedMean_Raw']\n",
    "\n",
    "# Plot heatmaps of correlation for each species\n",
    "for isp, species in enumerate(species_data):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    fig, ax = plt.subplots(ncols=1, figsize=(4, 4))\n",
    "    data = sat.load_pickle('training_data', species)# Adjust size based on the number of metrics\n",
    "    corr_matrix = data[selected_metrics].corr()  # Compute correlation matrix for selected metrics\n",
    "    sns.heatmap(corr_matrix,ax=ax, annot=True, fmt=\".2f\", cmap=\"BuPu\", vmin=-0.1, vmax=1,cbar=False)\n",
    "\n",
    "    # Remove ticks and labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot (if necessary)\n",
    "    sat.save_plot('cross_species', project, filename=f\"metrics_correlation_heatmap_{species}.svg\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE S.3. METRICS SCATTERPLOTS\n",
    "\n",
    "# Define the pairs to plot\n",
    "metric_pairs = [('Correct', 'Bias'), ('Correct', 'Prec'), ('Bias', 'Prec')]\n",
    "\n",
    "metric_pairs += [(x, y) for y in ['Correct', 'Bias', 'Prec'] for x in ['RT', 'SpeedMean']]\n",
    "\n",
    "# Add ('RT', 'SpeedMean') for a total of 10 plots\n",
    "metric_pairs.append(('RT', 'SpeedMean'))\n",
    "\n",
    "for species in ['human', 'monkey', 'mouse']:\n",
    "    data = sat.load_pickle(f'training_data', species)\n",
    "    \n",
    "    # Create a figure with subplots for the 10 unique pairs in a 2x5 grid\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    #fig.suptitle(f'Metric Clouds for {species}', fontsize=16)\n",
    "    \n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    label_map = {\n",
    "        'Correct': 'Hit rate',\n",
    "        'Prec': 'Precision',\n",
    "        'SpeedMean': 'Speed'\n",
    "    }\n",
    "    \n",
    "    # Loop through each specified pair of metrics\n",
    "    for idx, (metric_x, metric_y) in enumerate(metric_pairs):\n",
    "        ax = axes[idx]\n",
    "        sns.scatterplot(data=data, x=metric_x, y=metric_y, hue=data['States'], \n",
    "                        palette=colormap_states, alpha=0.8, legend=False, ax=ax)\n",
    "        ax.set_xlabel(label_map.get(metric_x, metric_x), fontsize=18)\n",
    "        ax.set_ylabel(label_map.get(metric_y, metric_y), fontsize=18)\n",
    "        ax.tick_params(axis='both', labelsize=14)\n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to make room for the main title\n",
    "    sat.save_plot('cross_species', project, filename=f\"organized_metric_clouds_{species}.svg\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE S.4. EFFECT OF SMOOTHING WINDOWSIZE\n",
    "preprocessed_data = sat.read_dfs(folder=project)\n",
    "\n",
    "# Define window sizes and initialize storage lists\n",
    "windows = np.arange(2, 20, 1)\n",
    "n_runs = 20\n",
    "n_states = 4\n",
    "accuracy_metrics = ['Correct', 'Bias', 'Prec']\n",
    "speed_metrics = ['RT', 'SpeedMean']\n",
    "\n",
    "try:\n",
    "    with open('species_results1.pkl', 'rb') as f:\n",
    "        species_results = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    species_results = {}\n",
    "\n",
    "for species in preprocessed_data:\n",
    "    if species in species_results:\n",
    "        continue  # Skip already processed species\n",
    "    sessions = preprocessed_data[species]\n",
    "\n",
    "    median_dwell = []\n",
    "    perc_5_dwell = []\n",
    "    perc_95_dwell = []\n",
    "\n",
    "    median_dwell_acc = []\n",
    "    perc_5_dwell_acc = []\n",
    "    perc_95_dwell_acc = []\n",
    "\n",
    "    median_dwell_sp = []\n",
    "    perc_5_dwell_sp = []\n",
    "    perc_95_dwell_sp = []\n",
    "\n",
    "    for window in windows:\n",
    "        run_dwell = []\n",
    "        run_dwell_acc = []\n",
    "        run_dwell_sp = []\n",
    "\n",
    "        for run in range(n_runs):\n",
    "            columns, values = prepare_and_train_dataset(1,sessions, accuracy_metrics, speed_metrics, shuffle=False)\n",
    "            training_data = pd.DataFrame(values, columns=columns)\n",
    "            \n",
    "            states=training_data['States']\n",
    "            states_1=training_data['States_Accuracy']\n",
    "            states_2=training_data['States_Speed']\n",
    "            z, p, d, values = sat.rle_with_delays(states)\n",
    "            z_acc, p, d, values_acc = sat.rle_with_delays(states_1)\n",
    "            z_sp, p, d, values_sp = sat.rle_with_delays(states_2)\n",
    "\n",
    "\n",
    "            # Store dwell times for this run, using np.nan for missing states\n",
    "            max_states = 2  # Set max_states to the total number of expected states\n",
    "            run_dwell.append([np.median(z[values == state]) if state in values else np.nan for state in range(max_states*2)])\n",
    "            run_dwell_acc.append([np.median(z_acc[values_acc == state]) if state in values_acc else np.nan for state in range(max_states)])\n",
    "            run_dwell_sp.append([np.median(z_sp[values_sp == state]) if state in values_sp else np.nan for state in range(max_states)])\n",
    "\n",
    "\n",
    "        # Calculate median and percentiles across runs for each state\n",
    "        run_dwell = np.array(run_dwell)\n",
    "        run_dwell_acc = np.array(run_dwell_acc)\n",
    "        run_dwell_sp = np.array(run_dwell_sp)\n",
    "\n",
    "        median_dwell.append(np.nanmedian(run_dwell, axis=0))\n",
    "        perc_5_dwell.append(np.nanpercentile(run_dwell, 5, axis=0))\n",
    "        perc_95_dwell.append(np.nanpercentile(run_dwell, 95, axis=0))\n",
    "\n",
    "        median_dwell_acc.append(np.nanmedian(run_dwell_acc, axis=0))\n",
    "        perc_5_dwell_acc.append(np.nanpercentile(run_dwell_acc, 5, axis=0))\n",
    "        perc_95_dwell_acc.append(np.nanpercentile(run_dwell_acc, 95, axis=0))\n",
    "\n",
    "        median_dwell_sp.append(np.nanmedian(run_dwell_sp, axis=0))\n",
    "        perc_5_dwell_sp.append(np.nanpercentile(run_dwell_sp, 5, axis=0))\n",
    "        perc_95_dwell_sp.append(np.nanpercentile(run_dwell_sp, 95, axis=0))\n",
    "\n",
    "    # Store results in the dictionary\n",
    "    species_results[species] = {\n",
    "        'median_dwell': median_dwell,\n",
    "        'perc_5_dwell': perc_5_dwell,\n",
    "        'perc_95_dwell': perc_95_dwell,\n",
    "        'median_dwell_acc': median_dwell_acc,\n",
    "        'perc_5_dwell_acc': perc_5_dwell_acc,\n",
    "        'perc_95_dwell_acc': perc_95_dwell_acc,\n",
    "        'median_dwell_sp': median_dwell_sp,\n",
    "        'perc_5_dwell_sp': perc_5_dwell_sp,\n",
    "        'perc_95_dwell_sp': perc_95_dwell_sp,\n",
    "    }\n",
    "    # Save the updated results after each species to prevent data loss\n",
    "    #sat.save_pickle(data,f'species_results',species)\n",
    "    with open('species_results.pkl', 'wb') as f:\n",
    "        pickle.dump(species_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE S.4 EFFECT OF SMOOTHING WINDOWSIZE PLOT\n",
    "\n",
    "with open('species_results.pkl', 'rb') as f:\n",
    "    species_results = pickle.load(f)\n",
    "windows = np.arange(2, 20, 1)   \n",
    "values_acc = [0,1]\n",
    "values_sp = [0,1]\n",
    "for species, results in species_results.items():\n",
    "    median_dwell = results['median_dwell']\n",
    "    perc_5_dwell = results['perc_5_dwell']\n",
    "    perc_95_dwell = results['perc_95_dwell']\n",
    "\n",
    "    median_dwell_acc = results['median_dwell_acc']\n",
    "    perc_5_dwell_acc = results['perc_5_dwell_acc']\n",
    "    perc_95_dwell_acc = results['perc_95_dwell_acc']\n",
    "\n",
    "    median_dwell_sp = results['median_dwell_sp']\n",
    "    perc_5_dwell_sp = results['perc_5_dwell_sp']\n",
    "    perc_95_dwell_sp = results['perc_95_dwell_sp']\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "    states = [0,1,2,3]#np.unique(values)\n",
    "\n",
    "    for i, state in enumerate(states):\n",
    "        median_dwell_state = [md[i] for md in median_dwell]\n",
    "        perc_5_dwell_state = [p5[i] for p5 in perc_5_dwell]\n",
    "        perc_95_dwell_state = [p95[i] for p95 in perc_95_dwell]\n",
    "        axs[0].plot(windows, median_dwell_state, '-o', label=f'State {state}', color=colormap_states(int(state)))\n",
    "        axs[0].fill_between(windows, perc_5_dwell_state, perc_95_dwell_state, color=colormap_states(int(state)), alpha=0.2)\n",
    "\n",
    "    states_acc = np.unique(values_acc)\n",
    "    for i, state in enumerate(states_acc):\n",
    "        median_dwell_acc_state = [md[i] for md in median_dwell_acc]\n",
    "        perc_5_dwell_acc_state = [p5[i] for p5 in perc_5_dwell_acc]\n",
    "        perc_95_dwell_acc_state = [p95[i] for p95 in perc_95_dwell_acc]\n",
    "        axs[1].plot(windows, median_dwell_acc_state, '-o', label=f'State {state}', color=colormap_states_accuracy(int(state)))\n",
    "        axs[1].fill_between(windows, perc_5_dwell_acc_state, perc_95_dwell_acc_state, color=colormap_states_accuracy(int(state)), alpha=0.2)\n",
    "\n",
    "    states_sp = np.unique(values_sp)\n",
    "    for i, state in enumerate(states_sp):\n",
    "        median_dwell_sp_state = [md[i] for md in median_dwell_sp]\n",
    "        perc_5_dwell_sp_state = [p5[i] for p5 in perc_5_dwell_sp]\n",
    "        perc_95_dwell_sp_state = [p95[i] for p95 in perc_95_dwell_sp]\n",
    "        axs[2].plot(windows, median_dwell_sp_state, '-o', label=f'State {state}', color=colormap_states_speed(int(state)))\n",
    "        axs[2].fill_between(windows, perc_5_dwell_sp_state, perc_95_dwell_sp_state, color=colormap_states_speed(int(state)), alpha=0.2)\n",
    "\n",
    "    axs[0].set_title(f'All States ({species})')\n",
    "    axs[0].set_ylim(0, 25)\n",
    "    axs[1].set_title(f'States Accuracy ({species})')\n",
    "    axs[1].set_ylim(0, 45)\n",
    "    axs[2].set_title(f'States Speed ({species})')\n",
    "    axs[2].set_ylim(0, 45)\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('Window Size')\n",
    "        ax.set_ylabel('Median Dwell Time')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sat.save_plot('cross_species', project, filename=f\"smoothing_win_dwell_time_{species}.svg\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE S.5. PSTH OF STATE OCCURANCE\n",
    "\n",
    "window = 15\n",
    "gap=1\n",
    "num_states = 4\n",
    "\n",
    "# Create figure outside the loop\n",
    "for i, species in enumerate (species_data):\n",
    "    fig, axs = plt.subplots(num_states, num_states, figsize=(9, 9))  # Adjust size as needed\n",
    "    fig.suptitle(f'State PSTH {species}')\n",
    "    \n",
    "    data = sat.load_pickle(f'training_data', species)\n",
    "    states = data['States']\n",
    "    z, p, v = sat.rle(states)\n",
    "    \n",
    "    all_psth = []\n",
    "\n",
    "    # Run-length encoding for each species\n",
    "    z, p, v = sat.rle(data['States'])\n",
    "    for s1 in range(num_states):\n",
    "        for s2 in range(num_states):\n",
    "            psth = sat.psth_state_occurrence(states, z, p, v, s1, s2, window, gap=gap)\n",
    "            all_psth.append(psth)\n",
    "\n",
    "    global_max = np.max([np.max(arr) for arr in all_psth])\n",
    "    \n",
    "    for state_1 in range(num_states):\n",
    "        for state_2 in range(num_states):\n",
    "            psth = sat.psth_state_occurrence(states, z, p, v, state_1, state_2, window,gap=gap)\n",
    "            psth_norm = psth / global_max\n",
    "            # Select the appropriate subplot\n",
    "            ax = axs[state_1, state_2]\n",
    "            \n",
    "            # Plot on the selected subplot\n",
    "            #ax.bar(np.arange(-window-gap/2, window + gap/2), psth, color=colormap_states.colors[state_2], width=1 )# , edgecolor=colormap_states.colors[state_2])\n",
    "            #ax.bar(np.arange(-gap/2,gap/2), ylim, color=colormap_states.colors[state_1], width=1)\n",
    "            \n",
    "            ax.bar(np.arange(-window ,window+1), psth_norm, color=colormap_states.colors[state_2], width=1 )# , edgecolor=colormap_states.colors[state_2]),linewidth=0\n",
    "            ax.axvline(0,linewidth=3, color=colormap_states.colors[state_1])   \n",
    "            ax.set_ylim([0,1])\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    #sat.save_plot('cross_species', project, filename=f\"state_occurrence_psth_{species}_state_start.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGURE S.6. TRIAL DURATIONS\n",
    "n_species = len(species_data)\n",
    "\n",
    "# Set up a compact figure with subplots, one for each species\n",
    "fig, axes = plt.subplots(1, n_species, figsize=(7, 3), sharey=True)\n",
    "\n",
    "# Loop through each species and plot the trial duration distribution\n",
    "for i, species in enumerate(species_data):\n",
    "    # Load the data for the current species\n",
    "    data = sat.load_pickle('training_data', species)\n",
    "    \n",
    "    # Extract trial duration data and filter out trials longer than 50\n",
    "    state_durations = data['TrialDuration']\n",
    "    #state_durations = state_durations[state_durations <= 50]  # Remove trials > 50\n",
    "    \n",
    "    # Plot the histogram for each species in its respective subplot\n",
    "    sns.histplot(state_durations, kde=True, bins=30, ax=axes[i], color='C'+str(i))\n",
    "\n",
    "    # Set x-axis limit to 50 for all subplots\n",
    "    axes[i].set_xlim(0, 10)\n",
    "    \n",
    "    # Set labels and title for each subplot\n",
    "    axes[i].set_xlabel('Trial Duration (sec)')\n",
    "\n",
    "# Set the shared y-axis label\n",
    "axes[0].set_ylabel('Trial count')\n",
    "\n",
    "# Adjust layout to make the figure compact\n",
    "plt.tight_layout()\n",
    "sat.save_plot('cross_species', project, filename=f\"hist_trial_duration.svg\")\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
